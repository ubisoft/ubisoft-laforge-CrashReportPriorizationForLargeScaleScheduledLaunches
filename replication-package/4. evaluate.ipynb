{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2ac16-705e-4af1-a142-1f61798b3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "sim_t = 0.99\n",
    "t_g1_total_count = \n",
    "t_g1_daily_max_count = \n",
    "t_g2_total_count = \n",
    "t_g2_daily_max_count = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b9583-8c37-4b0b-8951-b9d95b6ead87",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = r\"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26200c4-ee72-4a69-9bed-dc6487d4becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2 = pd.read_csv(home+'g1_tabular_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d05882-4766-4206-ba43-e1bcce8c7e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truths = tabular_data_p2[(tabular_data_p2['total_count']>=t_g2_total_count) | (tabular_data_p2['daily_max_count']>=t_g2_daily_max_count)]\n",
    "ground_truths['post_release_count'] = ground_truths['total_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929616d-848e-48ef-8584-e949aa98d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_g2_text_embed_similar_pairs_processed_dir = home+\"/output/embed-similarity-pairs-processed/\"  \n",
    "similar_data = pd.read_json(g1_g2_text_embed_similar_pairs_processed_dir+\"merged_data_0.99_updated.json\")\n",
    "similar_data.columns = ['Project1CrashType','Project2CrashType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c97db-fd97-42de-83ac-0139c73e59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_tabular_data = pd.read_csv(home+'g2_tabular_data_400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1e052-5e6e-407d-a14c-db04aa6830b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_tabular_data = g1_tabular_data[['CrashType','total_count',\"daily_max_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea6257-1bdb-4cc3-9245-119bacec3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_g1_tabular_data = g1_tabular_data[(g1_tabular_data['total_count']>=t_g1_total_count)|(g1_tabular_data['daily_max_count']>=t_g1_daily_max_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19773f9-292c-487e-b182-00e125bd3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_g1_similar_data = similar_data.merge(viral_g1_tabular_data, left_on='Project2CrashType', right_on='CrashType')\n",
    "viral_g1_similar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a5b65-3d0a-42e2-829b-ef504f7f03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_df_counts_daily = pd.read_csv(home+\"occurrence_df_counts_daily.csv\")\n",
    "g2_release_date = 'xxxx-xx-xx'\n",
    "occurrence_df_counts_daily = occurrence_df_counts_daily[occurrence_df_counts_daily['project_key']=='g2']\n",
    "pre_release_occurrences = occurrence_df_counts_daily[occurrence_df_counts_daily['CreationDateObj']<g2_release_date].groupby(['CrashType'])[['ReportIds']].sum().reset_index()\n",
    "pre_release_counts = pre_release_occurrences.copy()\n",
    "pre_release_counts.columns = ['CrashType','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd781397-b816-4235-aab5-8cf4540234c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_only_suggestions = viral_g1_similar_data['Project1CrashType'].unique()\n",
    "len(sim_only_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b0bdf-f7e9-483e-913c-68212ace97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truths[ground_truths['CrashType'].map(lambda x: x in sim_only_suggestions)]), ground_truths[ground_truths['CrashType'].map(lambda x: x in sim_only_suggestions)]['post_release_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d14114-ad98-4ddf-a772-f12acf41ffb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabular_data_p2 = pd.read_csv(home+'g1_tabular_data.csv')\n",
    "tabular_data_p2 = tabular_data_p2[['CrashType', 'total_count', 'daily_max_count']]\n",
    "suggestions = []\n",
    "\n",
    "# Define the folder path\n",
    "g2_suggestions_dir = home+\"scores-imbalance-platform-exception-400-components/\"\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(g2_suggestions_dir):\n",
    "    try:\n",
    "\n",
    "        file_path = g2_suggestions_dir + filename \n",
    "        if '-False-' not in file_path: \n",
    "            print(filename)\n",
    "            df = pd.read_json(file_path)#[['suggestions']]\n",
    "            \n",
    "            print(df['precision'])\n",
    "            df = df[df['recall'].map(lambda x: x>0)&df['precision'].map(lambda x: x>0)]\n",
    "            for loc in df.loc:\n",
    "                try:\n",
    "                    print(loc['suggestions'].values())\n",
    "                    col = '-'.join(filename.split('.json')[0].split('-')[-2:])+'-'+loc['data']+'|g2|'\n",
    "                    tabular_data_p2[col+'predictions'] = tabular_data_p2['CrashType'].map(lambda x: x in loc['suggestions'].values())\n",
    "                    print(\"done\")\n",
    "                except Exception as e:\n",
    "                    print(\"here1\", e)\n",
    "                    pass\n",
    "    except Exception as e:\n",
    "        print(\"here2\", e)\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613ef85-a56f-4503-9523-edb1436ec0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_json(home+\"1.2 Get-similar-CrashType-data-v2-0.99-similar-groups.json\")\n",
    "d['data_similar_group'] = d['g2_CrashTypes']\n",
    "g2_similar_groups =  d[['data_similar_group']]\n",
    "\n",
    "dfs = []\n",
    "for ground_truth in ground_truths['CrashType'].values:\n",
    "    dfs.append(g2_similar_groups[g2_similar_groups['data_similar_group'].map(lambda li: ground_truth in li)])\n",
    "df = pd.concat(dfs)\n",
    "all_g2_ground_truth_CrashTypes = [k for CrashTypes in df['data_similar_group'].values.tolist() for k in CrashTypes]\n",
    "all_g2_ground_truth_CrashTypes = list(set(all_g2_ground_truth_CrashTypes+ground_truths['CrashType'].values.tolist()))\n",
    "all_g2_ground_truth_CrashTypes = pd. DataFrame({\"CrashType\":all_g2_ground_truth_CrashTypes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510a192-70ab-41a0-8fe6-c9677cab68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aeb6dd-f368-488a-a80a-baba76cc2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bd15d-3a4d-4ff0-8b6f-380248b67e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2['ml_pred_aggregated'] = tabular_data_p2[[c for c in tabular_data_p2.columns if 'predictions' in c]].apply(lambda x: Counter(x.values).most_common(1)[0][0], axis=1)\n",
    "tabular_data_p2['sim_pred_aggregated'] = tabular_data_p2['CrashType'].map(lambda x: x in sim_only_suggestions)\n",
    "tabular_data_p2['ml_plus_sim_pred_aggregated'] = tabular_data_p2['ml_pred_aggregated']&tabular_data_p2['sim_pred_aggregated']\n",
    "tabular_data_p2['ml_or_sim_pred_aggregated'] = tabular_data_p2['ml_pred_aggregated']|tabular_data_p2['sim_pred_aggregated']\n",
    "ground_truths_CrashTypes  = ground_truths['CrashType'].values\n",
    "tabular_data_p2['y_true']  = tabular_data_p2['CrashType'].map(lambda x: x in ground_truths_CrashTypes)\n",
    "tabular_data_p2['y_relaxed_true']  = tabular_data_p2['CrashType'].map(lambda x: x in all_g2_ground_truth_CrashTypes['CrashType'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a35ea-0186-4f63-8360-df8e7391ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_similar_groups['group_number'] = [i for i in range(len(g2_similar_groups))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1b351-a3f4-4931-aee1-187b63f1d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_similar_groups_explode = g2_similar_groups.explode('data_similar_group')\n",
    "g2_similar_groups_explode.columns = ['CrashType', 'similar_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37d004-f59e-4efb-ad5a-8dcb384c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2 = tabular_data_p2.merge(g2_similar_groups_explode, on=\"CrashType\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3347aa7-ae27-49c9-8eb5-397ba32112f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2['processed_similar_group'] = tabular_data_p2.apply(lambda r: r['CrashType'] if str(r['similar_group'])=='nan' else r['similar_group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe06df-b7b0-4076-915d-598f1c0420b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_release_counts = pre_release_counts.merge(g2_similar_groups_explode, on=\"CrashType\", how=\"left\")\n",
    "pre_release_counts['processed_similar_group'] = pre_release_counts.apply(lambda r: r['CrashType'] if str(r['similar_group'])=='nan' else r['similar_group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed52b9-4227-4bc7-810b-a9c125f18f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2_ = tabular_data_p2.copy()\n",
    "tabular_data_p2_['post_release_count'] = tabular_data_p2_['total_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba61831-0495-45a9-994a-efaf339c811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "ml_li = []\n",
    "ml_relaxed_li = []\n",
    "\n",
    "auc_baseline = 0.5\n",
    "auprc_baseline = len(tabular_data_p2[tabular_data_p2['y_true']])/len(tabular_data_p2)\n",
    "\n",
    "for count_t in range(0,10):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'ml_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                  \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "\n",
    "\n",
    "sim_li = []\n",
    "sim_relaxed_li = []\n",
    "\n",
    "for count_t in range(0,10):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    sim_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                   \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "    \n",
    "\n",
    "\n",
    "ml_sim_li = []\n",
    "ml_sim_relaxed_li = []\n",
    "\n",
    "for count_t in range(0,10):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'ml_plus_sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_sim_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "        \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "    \n",
    "ml_or_sim_li = []\n",
    "ml_or_sim_relaxed_li = []\n",
    "\n",
    "for count_t in range(0,10):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'ml_or_sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_or_sim_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "             \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "    \n",
    "    y_true_col = 'y_relaxed_true'\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_or_sim_relaxed_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb6dfd-7304-4679-acfd-0f6f0430ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the data\n",
    "\n",
    "metrics = {\n",
    "\"precision\": \"Precision\",\n",
    " \"recall\":\"Recall\",\n",
    " \"auc\":\"AUROC\",\n",
    "    \"auprc\": \"AUPRC\",\n",
    "    \"mcc\":\"MCC\"\n",
    "}\n",
    "\n",
    "for metric in metrics.keys():\n",
    "    print(metric)\n",
    "    \n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot([o[metric] for o in ml_li], color = 'black', label='ML - '+metrics[metric], linestyle = '-.')\n",
    "    \n",
    "    plt.plot([o[metric] for o in sim_li], color = 'black', label='Similarity - '+metrics[metric], linestyle = '--')\n",
    "    \n",
    "    plt.plot([o[metric] for o in ml_sim_li], color = 'black', label='ML & Similarity - '+metrics[metric], linestyle = '-')\n",
    "\n",
    "    if metric =='auc':\n",
    "        plt.axhline(y=auc_baseline, color='black', linestyle='dotted', label='Baseline '+metrics[metric])\n",
    "    elif metric =='auprc':\n",
    "        plt.axhline(y=auprc_baseline, color='black', linestyle='dotted', label='Baseline '+metrics[metric])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Rotate x labels\n",
    "    # plt.xticks(rotation=45)\n",
    "    #plt.legend(bbox_to_anchor=(0.9, -0.5))\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Pre-release Count Threshold')\n",
    "    plt.ylabel(metrics[metric])\n",
    "\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "    plt.savefig('g1-g2-recall.pdf', bbox_inches='tight')  # Save with tight layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a6b46-405c-4b35-8a40-d972759e0329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "ml_li = []\n",
    "ml_relaxed_li = []\n",
    "\n",
    "auc_baseline = 0.5\n",
    "auprc_baseline = len(tabular_data_p2[tabular_data_p2['y_true']])/len(tabular_data_p2)\n",
    "\n",
    "for count_t in range(0,1):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'ml_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "\n",
    "\n",
    "sim_li = []\n",
    "sim_relaxed_li = []\n",
    "\n",
    "for count_t in range(0,1):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    sim_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "    \n",
    "\n",
    "\n",
    "ml_sim_li = []\n",
    "ml_sim_relaxed_li = []\n",
    "\n",
    "for count_t in range(0,1):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'ml_plus_sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_sim_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "    \n",
    "ml_or_sim_li = []\n",
    "ml_or_sim_relaxed_li = []\n",
    "\n",
    "for count_t in range(0,1):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'ml_or_sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_or_sim_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })\n",
    "    \n",
    "    y_true_col = 'y_relaxed_true'\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index().merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index().merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    ml_or_sim_relaxed_li.append({\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641c0dc-6989-4416-b5be-0675889003a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd. DataFrame([ml_li[0],sim_li[0], ml_sim_li[0],ml_or_sim_li[0]])\n",
    "d[\"method\"] = [\"ml_li\", \"sim_li\", \"ml_sim_li\",\"ml_or_sim_li\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3714921-3bf3-4c1a-8a55-667c170c55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = d[['method', 'precision', 'recall', 'accuracy', 'auc', 'auprc', 'mcc', 'TPs_occurrences', 'FPs_occurrences']]\n",
    "d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7005b24-9739-405f-ba8b-4891361b97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count_t in range(0,1):\n",
    "\n",
    "    pre_release_important_crashes = pre_release_counts[pre_release_counts['count']>count_t]['CrashType'].values\n",
    "\n",
    "\n",
    "    \n",
    "    y_true_col = 'y_true'\n",
    "    y_pred_col = 'sim_pred_aggregated'\n",
    "    \n",
    "    tabular_data_p2_ = tabular_data_p2.copy()\n",
    "    tabular_data_p2_['important'] = tabular_data_p2_['CrashType'].map(lambda x: x in pre_release_important_crashes)\n",
    "    tabular_data_p2_[y_pred_col] = tabular_data_p2_[y_pred_col] & tabular_data_p2_['important']\n",
    "    print(y_pred_col,len(tabular_data_p2_[tabular_data_p2_[y_pred_col]]), 'suggestions', len(tabular_data_p2_[tabular_data_p2_[y_pred_col]])/len(tabular_data_p2_)*100, \"%\")\n",
    "    auc = roc_auc_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    auprc = average_precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    precision = precision_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    recall = recall_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    accuracy = accuracy_score(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    mcc = matthews_corrcoef(tabular_data_p2_[y_true_col], tabular_data_p2_[y_pred_col])\n",
    "    TPs = tabular_data_p2_[tabular_data_p2_[y_true_col] & tabular_data_p2_[y_pred_col]]\n",
    "    FPs = tabular_data_p2_[(tabular_data_p2_[y_true_col]==False) & tabular_data_p2_[y_pred_col]]\n",
    "    FNs = tabular_data_p2_[(tabular_data_p2_[y_true_col]) & (tabular_data_p2_[y_pred_col]==False)]\n",
    "    TPs_occurrences = TPs.reset_index(#['CrashType'].map(lambda x: str(int(x)) if type(x)==type(0.0) else x).reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    FPs_occurrences = FPs.reset_index(#).merge(g2_similar_groups_explode, on=\"processed_similar_group\"\n",
    "    ).merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()\n",
    "    \n",
    "    sim_li = {\"auc\":auc,\n",
    "               \"auprc\":auprc, \n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "                   \"accuracy\": accuracy,\n",
    "               \"mcc\":mcc,\n",
    "               \"TPs\":TPs,\n",
    "               \"FPs\":FPs,\n",
    "               \"FNs\":FNs,\n",
    "               \"TPs_occurrences\": TPs_occurrences,\n",
    "               \"FPs_occurrences\": FPs_occurrences\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf680da-95af-4733-b2d9-bafa5c679611",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2_[tabular_data_p2_['y_true']].merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c64ff6-eb1f-4f53-802c-88632eac42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_p2_.merge(tabular_data_p2_, on=\"CrashType\")['post_release_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b2d62-a158-4aa3-877c-414364f02763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "U1, p = mannwhitneyu(sim_li['FNs']['total_count'], sim_li['TPs']['total_count'], method=\"exact\", alternative='less')\n",
    "print(U1,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e7cc9-aa0c-49df-8b98-637582c34a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "U1, p = mannwhitneyu(sim_li['FNs']['daily_max_count'], sim_li['TPs']['daily_max_count'], method=\"exact\", alternative='less')\n",
    "print(U1,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db57382-2af5-4e07-a016-2020eb927e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"count\": sim_li['FNs']['daily_max_count'].values,\n",
    "                   \"Type_of_count\":[\"Daily max count\" for i in range(len(sim_li['FNs']))],\n",
    "                   \"\":[\"FNs\" for i in range(len(sim_li['FNs']))]\n",
    "                  })._append(pd.DataFrame({\"count\": sim_li['FNs']['total_count'].values,\n",
    "                   \"Type_of_count\":[\"Total count\" for i in range(len(sim_li['FNs']))],\n",
    "                   \"\":[\"FNs\" for i in range(len(sim_li['FNs']))]\n",
    "                  }))._append(pd.DataFrame({\"count\": sim_li['TPs']['daily_max_count'].values,\n",
    "                   \"Type_of_count\":[\"Daily max count\" for i in range(len(sim_li['TPs']))],\n",
    "                   \"\":[\"TPs\" for i in range(len(sim_li['TPs']))]\n",
    "                  })\n",
    "                  )._append(pd.DataFrame({\"count\": sim_li['TPs']['total_count'].values,\n",
    "                   \"Type_of_count\":[\"Total count\" for i in range(len(sim_li['TPs']))],\n",
    "                   \"\":[\"TPs\" for i in range(len(sim_li['TPs']))]\n",
    "                  }))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63261e-708e-4c9f-bde5-738d3c635aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df_total_count = df[df['Type_of_count']=='Total count']\n",
    "normalized_arr = preprocessing.normalize([df_total_count['count']])\n",
    "df_total_count['normalized_count'] = normalized_arr[0]\n",
    "print(df_total_count)\n",
    "\n",
    "df_daily_max_count = df[df['Type_of_count']=='Daily max count']\n",
    "normalized_arr = preprocessing.normalize([df_daily_max_count['count']])\n",
    "df_daily_max_count['normalized_count'] = normalized_arr[0]\n",
    "\n",
    "normalized_df = df_total_count._append(df_daily_max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096ffe0-8f49-4ede-a9b1-32401049f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.columns = ['count', 'Type_of_count', '', 'Normalized count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd7843-55ed-48f4-ab8e-cc50954d3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = sns.violinplot(data=normalized_df, y=\"Type_of_count\", x=\"Normalized count\", hue=\"\", split=True, inner=\"quart\", scale='count', palette=[\"lightgrey\", \"darkgrey\"])\n",
    "plt.xscale('log')\n",
    "i=0\n",
    "for l in ax.lines:\n",
    "    if i in [1,4,7,10]:\n",
    "        l.set_linestyle('-')\n",
    "    else:\n",
    "        l.set_linestyle('--')\n",
    "    l.set_color('black')\n",
    "    l.set_alpha(0.8)\n",
    "    i=i+1\n",
    "ax.set_ylabel('')\n",
    "plt.savefig('nomalized_bean_plot.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa74d0-a065-4838-bc2f-787df58ddb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "ax = sns.violinplot(data=normalized_df, y=\"Type_of_count\", x=\"Normalized count\", hue=\"\", split=True, inner=\"quart\", scale='count', palette=[\"lightgrey\", \"darkgrey\"])\n",
    "plt.xscale('log')\n",
    "i=0\n",
    "for l in ax.lines:\n",
    "    if i in [1,4,7,10]:\n",
    "        l.set_linestyle('-')\n",
    "    else:\n",
    "        l.set_linestyle('--')\n",
    "    l.set_color('black')\n",
    "    l.set_alpha(0.8)\n",
    "    i=i+1\n",
    "ax.set_ylabel('')\n",
    "plt.savefig('nomalized_bean_plot.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908a6ff-ba78-4c0b-9076-d70f40ae083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cliffs_delta import cliffs_delta\n",
    "d, res = cliffs_delta(df_total_count[df_total_count[\"\"]=='FNs']['normalized_count'].values, \n",
    "                      df_total_count[df_total_count[\"\"]=='TPs']['normalized_count'].values\n",
    "                      \n",
    ")\n",
    "\n",
    "print(d,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f788cf6-e070-452c-969e-f916acdf9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cliffs_delta import cliffs_delta\n",
    "d, res = cliffs_delta(df_daily_max_count[df_daily_max_count[\"\"]=='FNs']['normalized_count'].values, \n",
    "                      df_daily_max_count[df_daily_max_count[\"\"]=='TPs']['normalized_count'].values\n",
    "                      \n",
    ")\n",
    "\n",
    "print(d,res)                      \n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
